{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß¨ BINND Complete Workflow - From Data to Decisions\n",
    "This notebook demonstrates the complete workflow for DNA-DNA binding prediction using a BINND-like model. You'll be guided through:\n",
    "1. Data preparation for your custom dataset.\n",
    "2. Model training and evaluation.\n",
    "3. Output analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Environment Setup ‚öôÔ∏è\n",
    "\n",
    "We're assuming you've successfully set up the Conda environment and are using it as your Jupyter kernel. This initial step is crucial!\n",
    "The cell below will help confirm your setup and get our inference demonstration running:\n",
    "\n",
    "1.  First, it changes the current working directory to `ROOT_DIR`. This ensures Python can properly locate all the necessary project files and scripts for the BINND model. Think of it like making sure you're in the right folder on your computer before running a program!\n",
    "2.  Next, it executes the `sample_inference.py` script. This script runs a quick test inference. If your environment is set up correctly, you should see the inference output printed below without any errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/gunavaran/Downloads/BINND\n",
      "Using device: cuda\n",
      "Model loaded successfully from /home/gunavaran/Downloads/BINND/inference_demo/BINND.pt\n",
      "\n",
      "--- Inference Results ---\n",
      "Sequence 1: AGCGATACGCCTTAACGTCT\n",
      "Sequence 2: AATGGCGAAGGGGATCGTTC\n",
      "Prediction: Bound, Probability: 0.9795\n"
     ]
    }
   ],
   "source": [
    "from utils.paths import ROOT_DIR\n",
    "%cd $ROOT_DIR\n",
    "!python inference_demo/sample_inference.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Preparing the Datasets üìä\n",
    "\n",
    "To get started, you'll use a sample data.csv file (included when you clone this GitHub repository) located in ROOT_DIR/data/demo/. This file must contain three columns:\n",
    "\n",
    "1. Seq1: Represents the first DNA sequence in a pair. **5' to 3` orientation**.\n",
    "2. Seq2: Represents the second DNA sequence in a pair. Important: **5' to 3` orientation**.\n",
    "3. Label: Indicates whether the sequence pair is binding or non-binding. Important: While the notebook assumes these column names for direct execution, the underlying source code can be readily adapted by users familiar with Python to accommodate different naming conventions.\n",
    "\n",
    "The provided `data.csv` file contains 100,000 sequence pairs and their corresponding labels. In this initial step, we'll split the dataset into training, validation, and testing sets using an 8:1:1 ratio, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df shape:  (800000, 3)\n",
      "test_df shape:  (100000, 3)\n",
      "val_df shape:  (100000, 3)\n"
     ]
    }
   ],
   "source": [
    "from utils.data_utils import stratified_split_dataset\n",
    "\n",
    "data_dir = ROOT_DIR / 'data' / 'demo'\n",
    "input_data_path = data_dir / 'data.csv'\n",
    "\n",
    "stratified_split_dataset(csv_file_path=input_data_path,\n",
    "                         label_column_name='Label',\n",
    "                         out_dir_path=data_dir,\n",
    "                         val_size=0.1,\n",
    "                         test_size=0.1,\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `stratified_split_dataset` function is designed to split your dataset into training, validation, and testing sets while preserving the original class distribution of the specified `label_column_name`. This ensures that each split (train, validation, test) proportionally represents the different categories (bound and unbound) present in your `Label` column, which is crucial for robust model training and evaluation, especially with imbalanced datasets.\n",
    "\n",
    "The function takes the `csv_file_path`, the `label_column_name` (e.g., 'Label'), an `out_dir_path` for saving, and optional `test_size` and `val_size` parameters. It then saves the resulting `train.csv`, `val.csv`, and `test.csv` files to the specified output directory. You should now find these newly generated files in your `data_dir`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Model Training üöÄ\n",
    "\n",
    "With our training, validation, and testing datasets ready, we're now set to train our BINND-like model!\n",
    "\n",
    "During training, we'll need to specify locations to save the resulting trained model (so you can reuse it later for inference) and to store the training logs. These logs are essential for tracking the model's performance, convergence, and other important metrics throughout the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "data_dir = ROOT_DIR / 'data' / 'demo'\n",
    "checkpoint_dir = ROOT_DIR / 'experiments' / 'checkpoints' / 'demo'\n",
    "log_dir = ROOT_DIR / 'experiments' / 'logs' / 'demo'\n",
    "\n",
    "# Check if data directory exists\n",
    "if not os.path.exists(data_dir):\n",
    "    raise FileNotFoundError(f\"Data directory does not exist: {data_dir}\")\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "os.makedirs(log_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this workflow, we have two model architectures available:\n",
    "1. BINND: This is the robust model fully described in the research article.\n",
    "2. BINND-Lite: A more compact and lightweight version of BINND.\n",
    "\n",
    "You can find the implementations for both of these architectures in `ROOT_DIR/src/networks/cnn.py`. For simplicity and faster execution within this notebook, we'll be using the BINND-Lite architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_name = \"BINNDLite\"\n",
    "# change the network name to BINND if you want to use BINND's architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before feeding our DNA sequences into the model, they need to be converted into a numerical format. We'll be using the 4xn_v2 encoding scheme for this, which is fully described in the associated research paper. You can find the implementation of this specific encoding method in  `ROOT_DIR/src/dataloader/datasets.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_name = \"4xn_v2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our primary goal for this immediate step is to train the model, we're setting up an important flags üö© here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_train = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring Training Hyperparameters ‚öôÔ∏è\n",
    "\n",
    "As the final preparation step before we kick off training, we need to configure our hyperparameters. These values are crucial as they control the learning process itself, influencing everything from how fast your model learns to how often it saves progress.\n",
    "\n",
    "Feel free to experiment with these values as you gain more experience, as tweaking them can significantly impact model performance and training time!\n",
    "\n",
    "Here are the hyperparameters we'll be using:\n",
    "- `max_seq_length` (int): The fixed length of input DNA sequences the model expects. The current implementation only supports 20 nucleotide long sequences.\n",
    "- `batch_size` (int): Number of samples processed in one go.\n",
    "- `log_interval` (int): How frequently (in batches) to print intermediate training/validation progress and log to files.\n",
    "- `learning_rate` (float): (Only for training) The step size for the optimizer during training.\n",
    "- `num_epochs` (int): (Only for training) The total number of full passes over the training dataset.\n",
    "- `patience` (int): (Only for training) For early stopping; number of epochs to wait for validation loss improvement before stopping training.\n",
    "\n",
    "We have kept the `num_epochs` small for faster completion of this demonstration. To potentially improve your model's performance, consider increasing this value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 20\n",
    "batch_size = 512\n",
    "learning_rate = 0.0004\n",
    "num_epochs = 2\n",
    "patience = 2\n",
    "log_interval = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launching the Training Process! üöÄ\n",
    "Now for the exciting part! This cell generates and executes the command that initiates the entire training process for our BINND-like model.\n",
    "\n",
    "We're carefully assembling a command line string (cmd_args) that passes all the configurations we just set (like data paths, network choice, and hyperparameters) to our main training script (main.py).\n",
    "\n",
    "Once constructed, we'll print the full command so you can see exactly what's being run, and then execute it. This is where your model will start learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd_args = [\n",
    "    \"python\", f\"{ROOT_DIR}/main.py\",\n",
    "    \"--data_dir\", str(data_dir),\n",
    "    \"--checkpoint_dir\", str(checkpoint_dir),\n",
    "    \"--log_dir\", str(log_dir),\n",
    "    \"--network_name\", network_name,\n",
    "    \"--encoder_name\", encoder_name,\n",
    "    \"--max_seq_length\", str(max_seq_length),\n",
    "    \"--batch_size\", str(batch_size),\n",
    "    \"--log_interval\", str(log_interval),\n",
    "    \"--learning_rate\", str(learning_rate),\n",
    "    \"--num_epochs\", str(num_epochs),\n",
    "    \"--patience\", str(patience),\n",
    "    \"--is_train\"\n",
    "]\n",
    "cmd = \" \".join(cmd_args)\n",
    "print(f\"Running command:\\n{cmd}\")\n",
    "os.system(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Complete! üéâ\n",
    "Congratulations! You've successfully trained your BINND-like model!\n",
    "\n",
    "Your trained model that achieved the best performance during training can now be found in your `checkpoint_dir` under the name `best_model.pt`. This is the file you'll use for making future predictions!\n",
    "\n",
    "Your `log_dir` is also now populated with several files that provide insights into your training session:\n",
    "- `runtime_info.json`: A JSON file containing information about the training run, such as execution time and memory consumption.\n",
    "- `train_log.csv`: A CSV file logging the training loss and accuracy per  `log_interval`.\n",
    "- `val_log.csv`: A CSV file logging the validation loss and accuracy per  `log_interval`.\n",
    "- `train_loss.png`: A plot visualizing the training loss over time.\n",
    "- `train_accuracy.png`: A plot visualizing the training accuracy over time.\n",
    "- `val_loss.png`: A plot visualizing the validation loss over time.\n",
    "- `val_accuracy.png`: A plot visualizing the validation accuracy over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "display(Image(filename=f\"{log_dir}/train_accuracy.png\"))\n",
    "display(Image(filename=f\"{log_dir}/train_loss.png\"))\n",
    "display(Image(filename=f\"{log_dir}/val_accuracy.png\"))\n",
    "display(Image(filename=f\"{log_dir}/val_loss.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Model Testing üß™\n",
    "\n",
    "Now that we've successfully trained our model, the next crucial step in our workflow is to evaluate its performance! This is where we see how well our BINND-like model generalizes to unseen data. \n",
    "\n",
    "We can reuse most of the variables and configurations we set up during the training phase. Here's what we'll be using:\n",
    "- Our dedicated test dataset (test.csv), which you'll find ready in your `data_dir`. This data was held back and never seen by the model during training, so it provides an unbiased evaluation. \n",
    "- The trained model (`best_model.pt`) is stored in the `checkpoint_dir`.\n",
    "- We'll continue to use the same `log_dir` to neatly store all the prediction results and evaluation metrics from this testing phase. \n",
    "- It's important to use the same `network_name` and `encoder_name` that were configured during the training phase to ensure compatibility with the trained model.\n",
    "- We can reuse most of the hyperparameters as well such as `max_seq_length`, `batch_size`, and `log_interval`.\n",
    "\n",
    "The main thing we should change is the following flag üö©:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_test = True # We're now in testing mode!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launching the Model Testing! üöÄ\n",
    "Similar to our training phase, it's time to assemble all the arguments and generate the command to run our model in testing mode. This command will tell our main.py script to use the trained model on the `test.csv` dataset.\n",
    "\n",
    "We're carefully building the command line string (`cmd_args`) here, incorporating all the paths, network configurations, and hyperparameters we've defined for testing. Notice that the `is_test` flag üö© is now set to `True`!\n",
    "\n",
    "Once constructed, we'll print the full command for your review and then execute it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd_args = [\n",
    "    \"python\", f\"{ROOT_DIR}/main.py\",\n",
    "    \"--data_dir\", str(data_dir),\n",
    "    \"--checkpoint_dir\", str(checkpoint_dir),\n",
    "    \"--log_dir\", str(log_dir),\n",
    "    \"--network_name\", network_name,\n",
    "    \"--encoder_name\", encoder_name,\n",
    "    \"--max_seq_length\", str(max_seq_length),\n",
    "    \"--batch_size\", str(batch_size),\n",
    "    \"--log_interval\", str(log_interval),\n",
    "    \"--is_test\"\n",
    "]\n",
    "\n",
    "cmd = \" \".join(cmd_args)\n",
    "print(f\"Running command:\\n{cmd}\")\n",
    "os.system(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Results! ‚úÖ\n",
    "\n",
    "After successfully completing the testing phase, your `log_dir` is now updated with new files containing the evaluation results. These will help you analyze the model's performance on unseen data.\n",
    "\n",
    "You'll find the following new files:\n",
    "- `roc_curve.csv`: A CSV file containing the data points used to plot the Receiver Operating Characteristic (ROC) curve.\n",
    "- `roc_curve.png`: A visual representation of the ROC curve, which is a powerful tool for evaluating the diagnostic ability of a binary classifier system as its discrimination threshold is varied.\n",
    "- `test_log.csv`: A CSV file logging predictions (column named Probability) and ground truth labels (column named Label) for each sample in the test set.\n",
    "- `test_metrics.json`: A JSON file summarizing key overall performance metrics on the test set, such as accuracy, precision, recall, F1-score, and AUC (Area Under the Curve).\n",
    "- `confusion_matrix.png`: A visual representation of the confusion matrix, which provides a detailed breakdown of correct and incorrect classifications for each class.\n",
    "\n",
    "Additionally, the `runtime_info.json` file will have been updated, with execution time and memory consumption from the testing phase appended to it. This helps you keep a comprehensive record of resource usage across different stages of your pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.203769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.293140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.156189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.211322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.715777</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label  Probability\n",
       "0    1.0     0.203769\n",
       "1    1.0     0.293140\n",
       "2    0.0     0.156189\n",
       "3    1.0     0.211322\n",
       "4    0.0     0.715777"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results_df = pd.read_csv(log_dir / 'test_log.csv')\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename=f\"{log_dir}/confusion_matrix.png\"))\n",
    "display(Image(filename=f\"{log_dir}/roc_curve.png\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
